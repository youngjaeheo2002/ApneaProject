{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_and_targets_test.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#load the pickle dictionary\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Read and deserialize from file\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data_and_targets_test.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    158\u001b[0m     arr \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    159\u001b[0m split_index \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(arr) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.6\u001b[39m)\n",
      "File \u001b[0;32m~/ENV/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_and_targets_test.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "class TimeSeriesCNN(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(TimeSeriesCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Updated the input size for the dense layer based on the conv layer calculations\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128 * 124, 512),  # Updated size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, K)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.conv_layers(X)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the output for the dense layer\n",
    "        out = self.dense_layers(out)\n",
    "        return out\n",
    "\n",
    "def stackData(names_data_and_targets):\n",
    "    res_data = np.empty((0,1000))\n",
    "    res_target = np.empty((0,1))\n",
    "\n",
    "    for item in names_data_and_targets:\n",
    "        data = item['data']\n",
    "        target = item['target']\n",
    "\n",
    "        res_data = np.vstack((res_data,data))\n",
    "        res_target = np.vstack((res_target,target))\n",
    "\n",
    "    return res_data,res_target\n",
    "\n",
    "def batch_gd(model,criterion,optimizer,train_loader,test_loader,epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "\n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "\n",
    "        for inputs,targets in train_loader:\n",
    "            \n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            targets = targets.squeeze()\n",
    "            print(inputs.shape,targets.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss= criterion(outputs,targets)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        #get train loss and test loss\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = []\n",
    "\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs,targets)\n",
    "\n",
    "            test_loss.append(loss.item())\n",
    "\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        #save losses\n",
    "\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "        Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "\n",
    "    return train_losses,test_losses, model\n",
    "def turnToBinary(targets):\n",
    "    res = np.zeros(shape = (targets.shape))\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if targets[i][0] > 0:\n",
    "            res[i][0] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, name,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    the following is test code'''\n",
    "    print('hello')\n",
    "\n",
    "    #load the pickle dictionary\n",
    "\n",
    "        # Read and deserialize from file\n",
    "    with open('./data_and_targets_test.pkl', 'rb') as f:\n",
    "        arr = pickle.load(f)\n",
    "    split_index = math.ceil(len(arr) * 0.6)\n",
    "\n",
    "    training_data = arr[:split_index]\n",
    "    testing_data = arr[split_index:]\n",
    "\n",
    "    training_data,training_target = stackData(training_data)\n",
    "    testing_data,testing_target = stackData(testing_data)\n",
    "\n",
    "    #turn this classification problem into binary classification\n",
    "    training_target = turnToBinary(training_target)\n",
    "    testing_target = turnToBinary(testing_target)\n",
    "\n",
    "    #loss and optimizer\n",
    "    model = TimeSeriesCNN(2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    #define batch size\n",
    "    batch_size = 128\n",
    "\n",
    "    #initilize training data and target tensors and dataset\n",
    "    training_data_tensor = torch.tensor(training_data,dtype = torch.float32)\n",
    "    training_target_tensor = torch.tensor(training_target,dtype=torch.long)\n",
    "    training_dataset = TensorDataset(training_data_tensor,training_target_tensor)\n",
    "\n",
    "    #create training dataloader\n",
    "    train_loader = DataLoader(dataset = training_dataset,batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    #initialize testing data and target tensors and dataset\n",
    "    testing_data_tensor = torch.tensor(testing_data,dtype = torch.float32)\n",
    "    testing_target_tensor = torch.tensor(testing_target,dtype = torch.long)\n",
    "    testing_dataset = TensorDataset(testing_data_tensor,testing_target_tensor)\n",
    "\n",
    "    #create testing dataloader\n",
    "    test_loader = DataLoader(dataset = testing_dataset,batch_size=batch_size,shuffle = True)\n",
    "    \n",
    "\n",
    "    #set epochs \n",
    "    epochs = 10\n",
    "\n",
    "\n",
    "    train_losses, test_losses, model = batch_gd(model=model,criterion=criterion,optimizer=optimizer,train_loader=train_loader,test_loader=test_loader,epochs=epochs)\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(),\"my_model.pt\")\n",
    "    \n",
    "    #Record accuracy\n",
    "    model.eval()\n",
    "    n_correct = 0.\n",
    "    n_total = 0.\n",
    "\n",
    "    for inputs,targets in train_loader:\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _,predictions = torch.max(outputs,1)\n",
    "\n",
    "        n_correct += (predictions == targets).sum().item()\n",
    "        n_total += targets.shape[0]\n",
    "\n",
    "    train_acc = n_correct/n_total\n",
    "\n",
    "    n_correct = 0.\n",
    "    n_total = 0.\n",
    "\n",
    "    for inputs,targets in test_loader:\n",
    "\n",
    "        outputs= model(inputs)\n",
    "\n",
    "        _,predictions= torch.max(outputs,1)\n",
    "\n",
    "        n_correct += (predictions == targets).sum().item()\n",
    "\n",
    "        n_total += targets.shape[0]\n",
    "\n",
    "    test_acc = n_correct/n_total\n",
    "\n",
    "    print(f\"Train acc : {train_acc:.4f}, Test acc : {test_acc:.4f}\")\n",
    "\n",
    "    #print confusion matrix\n",
    "\n",
    "    x_test = testing_dataset.data.numpy()\n",
    "    y_test = testing_dataset.targets.numpy()\n",
    "    p_test = np.array([])\n",
    "    for inputs, targets in test_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get prediction\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        # update p_test\n",
    "        p_test = np.concatenate((p_test, predictions.cpu().numpy()))\n",
    "\n",
    "        cm = confusion_matrix(y_test, p_test)\n",
    "        plot_confusion_matrix(cm, list(range(2)))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
